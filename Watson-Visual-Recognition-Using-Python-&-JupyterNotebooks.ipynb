{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Watson Visual Recognition Tutorial: Python & Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies\n",
    "This part represents how to use python programming and utilize the Watson visual recognition. The prerequisites will be &quot;Watson Developer Cloud SDK&quot;. In order to install it, use pip3 or easy\\_install :\n",
    "```python\n",
    "Pip3 install --upgrade watson-developer-cloud\n",
    "```\n",
    "or\n",
    "\n",
    "easy\\_install --upgrade watson-developer-cloud\n",
    "\n",
    "### Accessing the Visual Recognition Service\n",
    "In order to access the service (APIs) the credentials are required.  For getting the service credentials, the user needs to create the service (Visual Recognition Service) through the [IBM cloud](https://console.bluemix.net).\n",
    "\n",
    "1. Log in to the IBM cloud\n",
    "2. Go to the catalog\n",
    "3. Select Watson visual recognition service\n",
    "4. Choose the name and create the service\n",
    "5. Now in the next page the user can see the credentials &quot;apikey, etc.&quot;\n",
    "\n",
    "**Note:  ** Instantiation with API key works only with Visual Recognition service instances created before May 23, 2018. Visual Recognition instances created after May 22 use IAM.\n",
    "```python\n",
    "from watson_developer_cloud import VisualRecognitionV3\n",
    "# In the constructor\n",
    "visual_recognition = VisualRecognitionV3(version='2018-05-22', api_key='<api_key>')\n",
    "\n",
    "\n",
    "# After instantiation\n",
    "visual_recognition = VisualRecognitionV3(version='2018-05-22')\n",
    "visual_recognition.set_api_key('<api_key>')\n",
    "```\n",
    "After installing the IBM cloud SDK, the user can call the build-in service or create the custom classification from the python and use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection With Python and Jupyter Notebooks\n",
    "\n",
    "In this part the face detection service is used to detect the human faces. A python code for utilizing this service will be as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Jupyter Notebook Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting watson-developer-cloud\n",
      "  Downloading https://files.pythonhosted.org/packages/41/35/9c98ba1056163641c97f1416e882679c2da941abb95d37311b90980c5293/watson-developer-cloud-1.3.5.tar.gz (192kB)\n",
      "\u001b[K    100% |████████████████████████████████| 194kB 3.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: requests<3.0,>=2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: python_dateutil>=2.5.3 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Collecting autobahn>=0.10.9 (from watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/c9/7a/140264ec2c162bb22f91be76a11554f8ab0eda9bb2c775b6bc0dbbef0d4a/autobahn-18.6.1-py2.py3-none-any.whl (294kB)\n",
      "\u001b[K    100% |████████████████████████████████| 296kB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting Twisted>=13.2.0 (from watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/2a/e9e4fb2e6b2f7a75577e0614926819a472934b0b85f205ba5d5d2add54d0/Twisted-18.4.0.tar.bz2 (3.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.0MB 281kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: pyOpenSSL>=16.2.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from watson-developer-cloud)\n",
      "Collecting service-identity>=17.0.0 (from watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/29/fa/995e364220979e577e7ca232440961db0bf996b6edaf586a7d1bd14d81f1/service_identity-17.0.0-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests<3.0,>=2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: six>=1.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from python_dateutil>=2.5.3->watson-developer-cloud)\n",
      "Collecting txaio>=2.10.0 (from autobahn>=0.10.9->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/2e/c8a877b0a5c2798fa93ebcc1465a72a68c089e5f8b0a852ca335751dcc5a/txaio-2.10.0-py2.py3-none-any.whl\n",
      "Collecting zope.interface>=4.4.2 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/8a/657532df378c2cd2a1fe6b12be3b4097521570769d4852ec02c24bd3594e/zope.interface-4.5.0.tar.gz (151kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting constantly>=15.1 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
      "Collecting incremental>=16.10.1 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
      "Collecting Automat>=0.3.0 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/17/6a/1baf488c2015ecafda48c03ca984cf0c48c254622668eb1732dbe2eae118/Automat-0.6.0-py2.py3-none-any.whl\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=13.2.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/b6/84d0c863ff81e8e7de87cff3bd8fd8f1054c227ce09af1b679a8b17a9274/hyperlink-18.0.0-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: cryptography>=1.9 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Collecting pyasn1 (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/a0/70/2c27740f08e477499ce19eefe05dbcae6f19fdc49e9e82ce4768be0643b9/pyasn1-0.4.3-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 9.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting attrs (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/41/59/cedf87e91ed541be7957c501a92102f9cc6363c623a7666d69d51c78ac5b/attrs-18.1.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules (from service-identity>=17.0.0->watson-developer-cloud)\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/51/bcd96bf6231d4b2cc5e023c511bee86637ba375c44a6f9d1b4b7ad1ce4b9/pyasn1_modules-0.2.1-py2.py3-none-any.whl (60kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 8.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: setuptools in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from zope.interface>=4.4.2->Twisted>=13.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: asn1crypto>=0.21.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: cffi>=1.7 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Requirement not upgraded as not directly required: pycparser in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from cffi>=1.7->cryptography>=1.9->pyOpenSSL>=16.2.0->watson-developer-cloud)\n",
      "Building wheels for collected packages: watson-developer-cloud, Twisted, zope.interface\n",
      "  Running setup.py bdist_wheel for watson-developer-cloud ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/e4/21/d0/baf64c6565b076af23eb7e188bb5b55b12db4639bbc8aff27b\n",
      "  Running setup.py bdist_wheel for Twisted ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/b3/76/f7/85353c829c0881e74b5366ce0ed59042b098bb4903e2da8828\n",
      "  Running setup.py bdist_wheel for zope.interface ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/c6/b2/d2/be6785a207eaa58d76debc10c9d5c66196b40a88abb61d6af7\n",
      "Successfully built watson-developer-cloud Twisted zope.interface\n",
      "Installing collected packages: txaio, autobahn, zope.interface, constantly, incremental, attrs, Automat, hyperlink, Twisted, pyasn1, pyasn1-modules, service-identity, watson-developer-cloud\n",
      "Successfully installed Automat-0.6.0 Twisted-18.4.0 attrs-18.1.0 autobahn-18.6.1 constantly-15.1.0 hyperlink-18.0.0 incremental-17.5.0 pyasn1-0.4.3 pyasn1-modules-0.2.1 service-identity-17.0.0 txaio-2.10.0 watson-developer-cloud-1.3.5 zope.interface-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade watson-developer-cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from watson_developer_cloud import VisualRecognitionV3, WatsonApiException\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_url='https://www.runscope.com/static/img/public/customer-portrait-human-api.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy and paste your credentials in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials = {\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authenticate a Visual Recognition Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_recognition = VisualRecognitionV3(\n",
    "    version='2018-03-19',\n",
    "    url='https://gateway.watsonplatform.net/visual-recognition/api',\n",
    "    iam_api_key=credentials['apikey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_result = visual_recognition.detect_faces(parameters=json.dumps({'url': test_url}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images_processed\": 1,\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"face_location\": {\n",
      "            \"top\": 55,\n",
      "            \"height\": 119,\n",
      "            \"width\": 101,\n",
      "            \"left\": 95\n",
      "          },\n",
      "          \"age\": {\n",
      "            \"min\": 41,\n",
      "            \"score\": 0.8015439,\n",
      "            \"max\": 44\n",
      "          },\n",
      "          \"gender\": {\n",
      "            \"gender\": \"MALE\",\n",
      "            \"score\": 0.9999509\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"resolved_url\": \"https://www.runscope.com/static/img/public/customer-portrait-human-api.png\",\n",
      "      \"source_url\": \"https://www.runscope.com/static/img/public/customer-portrait-human-api.png\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(url_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More detail about “visual_recognition.detect_faces()” function:\n",
    "```python\n",
    "def detect_faces(self,\n",
    "                     images_file=None,\n",
    "                     parameters=None,\n",
    "                     images_file_content_type=None,\n",
    "                     images_filename=None,\n",
    "                     url=None,\n",
    "                     **kwargs);\n",
    "```\n",
    "Analyze and get data about faces in images. Responses can include estimated age and gender, and the service can identify celebrities. This feature uses a built-in classifier, so you do not train it on custom classifiers. The Detect faces method does not support general biometric facial recognition.\n",
    "\n",
    "        :param file images_file: An image file (.jpg, .png) or .zip file with images. Include no more than 15 images. You can also include images with the `url` property in the **parameters** object.  All faces are detected, but if there are more than 10 faces in an image, age and gender confidence scores might return scores of 0.\n",
    "        :param str parameters: (Deprecated) A JSON object that specifies a single image (.jpg, .png) to analyze by URL. The parameter can be sent as a string or a file.  Example: `{\\\"url\\\":\\\"http://www.example.com/images/myimage.jpg\\\"}`.\n",
    "        :param str images_file_content_type: The content type of images_file.\n",
    "        :param str images_filename: The filename for images_file.\n",
    "        :param str url: The URL of an image to analyze. Must be in .gif, .jpg, .png, or .tif format. The minimum recommended pixel density is 32X32 pixels per inch, and the maximum image size is 10 MB. Redirects are followed, so you can use a shortened URL.  You can also include images with the **images_file** parameter.\n",
    "        :param dict headers: A `dict` containing the request headers\n",
    "        :return: A `dict` containing the `DetectedFaces` response.\n",
    "        :rtype: dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matplotlib or the other libraries can be used to plot the results. In order to install Matplotlib visit [Matplolib installation instructions](https://matplotlib.org/users/installing.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "fig,ax = plt.subplots(1)\n",
    "for i in range(0,len(url_result['images'][0]['faces'])):\n",
    "    face=url_result['images'][0]['faces'][i]\n",
    "    faceloc = face['face_location']\n",
    "    x,y,w,h = faceloc['left'],faceloc['top'],faceloc['width'],faceloc['height']\n",
    "# Create a Rectangle patch\n",
    "    rect = patches.Rectangle((x,y),w,h,linewidth=1,edgecolor='r',facecolor='none')\n",
    "    #img=mpimg.imread('your_image.png')\n",
    "    ax.add_patch(rect)\n",
    "    # Create figure and axes\n",
    "\n",
    "img = mpimg.imread(test_url)\n",
    "ax.imshow(img)\n",
    "plt.show()\n",
    "```\n",
    "By running the code the result will be as follows:\n",
    "``` python\n",
    "{\n",
    "  \"images\": [\n",
    "    {\n",
    "      \"faces\": [\n",
    "        {\n",
    "          \"age\": {\n",
    "            \"min\": 41,\n",
    "            \"max\": 44,\n",
    "            \"score\": 0.8015439\n",
    "          },\n",
    "          \"face_location\": {\n",
    "            \"height\": 119,\n",
    "            \"width\": 101,\n",
    "            \"left\": 95,\n",
    "            \"top\": 55\n",
    "          },\n",
    "          \"gender\": {\n",
    "            \"gender\": \"MALE\",\n",
    "            \"score\": 0.9999509\n",
    "          }\n",
    "        }\n",
    "      ],\n",
    "      \"source_url\": \"https://www.runscope.com/static/img/public/customer-portrait-human-api.png\",\n",
    "      \"resolved_url\": \"https://www.runscope.com/static/img/public/customer-portrait-human-api.png\"\n",
    "    }\n",
    "  ],\n",
    "  \"images_processed\": 1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![](https://raw.github.ibm.com/Evan-Woods/Tutorials/master/VisualRec_Images/Face_Rec_Python.PNG?token=AAKOj-871jg6nSbvhmnmGA-vS9BtHx7aks5bJ-ehwA%3D%3D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Visual Recognition Custom classifier:\n",
    "\n",
    "This part presents the custom classifier using Watson Visual Recognition service. In order to start with the custom classifier, first the user need to prepare the images corresponding to each classes. (Ten images for each class) and put the images into the .zip file. Then the following python code will create the classes and train the model.\n",
    "```python\n",
    "from __future__ import print_function\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from watson_developer_cloud import VisualRecognitionV3, WatsonApiException\n",
    "\n",
    "with open('./beagle.zip', 'rb') as beagle, open(\n",
    "        './golden-retriever.zip', 'rb') as goldenretriever, open(\n",
    "            './husky.zip', 'rb') as husky, open(\n",
    "                './cats.zip', 'rb') as cats:\n",
    "    model = visual_recognition.create_classifier(\n",
    "        'dogs',\n",
    "        beagle_positive_examples=beagle,\n",
    "        goldenretriever_positive_examples=goldenretriever,\n",
    "        husky_positive_examples=husky,\n",
    "        negative_examples=cats)\n",
    "print(json.dumps(model, indent=2))\n",
    "```\n",
    "\n",
    "**The returned result from Watson will be:**\n",
    "\n",
    " ![](https://raw.github.ibm.com/Evan-Woods/Tutorials/master/VisualRec_Images/Custom_Classifier_Python.PNG?token=AAKOj_ZTYwLFZw2Na5HpJDJXlXIPnjpbks5bJ-g4wA%3D%3D)\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- The classifier\\_id is the ID that user can call the class and test it.\n",
    "- By each executing the code, Watson creates the classifier with the new classifier\\_id. So if the user wants to change the classifier, it is better to remove the previous classifier and train the model again. (Training process should be done only once)\n",
    "\n",
    "\n",
    "\n",
    "After training the model, in order to test the model the user can call the created classifier by using the following codes:\n",
    "```python\n",
    "from __future__ import print_function\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from watson_developer_cloud import VisualRecognitionV3, WatsonApiException\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "with open(location_of_image_on_hard_drive, 'rb') as images_file:\n",
    "\tresults_data  = visual_recognition.classify(\n",
    "\t\timages_file,\n",
    "\t\tthreshold='0.6',\n",
    "\t\tclassifier_ids='dogs_992830017')\n",
    "\tprint(json.dumps(results_data , indent=2))\n",
    "\n",
    "#The result from Watson will be as follows:\n",
    "\n",
    "classifiers_result =  results_data['images'][0]['classifiers']\n",
    "classes_result = classifiers_result[0]['classes']\n",
    "\n",
    "fig,ax = plt.subplots(1)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "img = mpimg.imread(location_of_image_on_hard_drive)\n",
    "cv2.putText(img,classes_result[0]['class'], (20,30), font, 1, (0, 0, 255), thickness=3)\n",
    "cv2.putText(img,str(classes_result[0]['score']), (20,60), font, 1, (0, 0, 255), thickness=3)\n",
    "\t\n",
    "ax.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    " ![](https://raw.github.ibm.com/Evan-Woods/Tutorials/master/VisualRec_Images/Custom_classification_Results_Dog.PNG?token=AAKOj8TvPSviCJm398QeYKi_jWDAAaBaks5bJ-iHwA%3D%3D)\n",
    "\n",
    "**Real-Time Face Detection:**\n",
    "\n",
    "The sample code for real time face detection will be as follows. In this code the camera frame is captured by the openCV library and then is applied to the built-in face detection. The returned results from the Watson in analyzed and plotted.\n",
    "\n",
    "```python\n",
    "from __future__ import print_function\n",
    "import json\n",
    "from os.path import join, dirname\n",
    "from watson_developer_cloud import VisualRecognitionV3, WatsonApiException\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import cv2  \n",
    "\n",
    "visual_recognition = VisualRecognitionV3(\n",
    "    version='2018-03-19',\n",
    "    url='https://gateway.watsonplatform.net/visual-recognition/api',\n",
    "    iam_api_key='…………')\n",
    "counter=0    \n",
    "camera = cv2.VideoCapture(0)\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "\n",
    "while(True):\n",
    "\n",
    "      return_value, image = camera.read()\n",
    "      cv2.imwrite('opencv'+str(0)+'.png', image)\n",
    "url_result                            =visual_recognition.detect_faces(images_file=open('opencv0.png','rb'))\n",
    "\n",
    "for i in range(0,len(url_result['images'][0]['faces'])):\n",
    "face=url_result['images'][0]['faces'][i]\n",
    "faceloc = face['face_location']\n",
    "x,y,w,h = faceloc['left'],faceloc['top'],faceloc['width'],faceloc['height']\n",
    "\t\t\n",
    "# Create a Rectangle patch around the detected face\n",
    "rect = cv2.rectangle(image,(x,y),(x+w,y+h),(0, 255, 0), thickness=3)\n",
    "\t\t\n",
    "# Add the corresponding face information to the image \n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.putText(image,face['gender']['gender'], (x,y+w), font, 1, (0, 255, 0),thickness=3)\n",
    "\t\t\n",
    "cv2.putText(image,\"Age (min):\", (x,y+w+30), font, 1, (0, 0, 255),thickness=3)\n",
    "\t\t\n",
    "cv2.putText(image,str(face['age']['min']), (x+180,y+w+30), font, 1, (0, 0, 255), thickness=3)\n",
    "\t\t\n",
    "cv2.putText(image,\"Age (max):\", (x,y+w+60), font, 1, (0, 0, 255),thickness=3)\n",
    "\n",
    "cv2.putText(image,str(face['age']['max']), (x+180,y+w+60), font, 1, (0, 0, 255), thickness=3)\n",
    "\n",
    "cv2.imshow(\"frame\", image)\n",
    "\n",
    "\n",
    "if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "\t\tbreak\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see an example of the Facial Recognition code in realtime, clone the repository and run the python code in the Watson-Face-and-Age-Recognition-using-Python.py example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
